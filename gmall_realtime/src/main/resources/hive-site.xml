<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- jdbc连接的URL -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://ecs2:3306/metastore?useSSL=false</value>
    </property>

    <!-- jdbc连接的Driver-->
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>

    <!-- jdbc连接的username-->
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>

    <!-- jdbc连接的password -->
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>sbc006688</value>
    </property>
    <!-- Hive默认在HDFS的工作目录 -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>

    <!-- Hive元数据存储版本的验证 -->
    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>
    <!-- 指定存储元数据要连接的地址  直连 vs 代理 -->
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://ecs2:9083</value>
    </property>
    <!-- 指定hiveserver2连接的端口号 -->
    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
    </property>
    <!-- 指定hiveserver2连接的host -->
    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>ecs2</value>
    </property>


        <!-- 元数据存储授权  -->
        <property>
            <name>hive.metastore.event.db.notification.api.auth</name>
            <value>false</value>
        </property>

        <!-- 打印当前库 和表头 -->
        <property>
            <name>hive.cli.print.header</name>
            <value>true</value>
            <description>Whether to print the names of the columns in query output.</description>
        </property>
        <property>
            <name>hive.cli.print.current.db</name>
            <value>true</value>
            <description>Whether to include the current database in the Hive prompt.</description>
        </property>

        <!-- 修改引擎 tez -->
        <property>
            <name>hive.execution.engine</name>
            <!-- <value>tez</value> -->
            <value>spark</value>
        </property>
        <property>
            <name>hive.tez.container.size</name>
            <!--<value>1024</value>-->
            <value>2018</value>
        </property>

        <!--Spark依赖位置-->
        <property>
            <name>spark.yarn.jars</name>
            <value>${fs.defaultFS}/spark-jars/*</value>
        </property>

        <!--Hive和spark连接超时时间-->
        <property>
            <name>hive.spark.client.connect.timeout</name>
            <value>10000ms</value>
        </property>

        <!-- 去除日志信息 -->
        <property>
            <name>hive.server2.logging.operation.enabled</name>
            <!-- <value>true</value> -->
            <value>false</value>
        </property>

        <!-- 新增zookeeper 属性  hbase -->
    <property>
        <name>hive.zookeeper.quorum</name>
        <value>ecs2,ecs3,ecs4</value>
        <description>The list of ZooKeeper servers to talk to. This is only needed for read/write locks.</description>
    </property>
    <property>
        <name>hive.zookeeper.client.port</name>
        <value>2181</value>
        <description>The port of ZooKeeper servers to talk to. This is only needed for read/write locks.</description>
    </property>


    <!-- 指定队列 -->
    <property>
        <name>mapreduce.job.queuename</name>
        <value>hive</value>
    </property>

    <!-- 指定input.format -->
    <property>
        <name>hive.input.format</name>
        <value>org.apache.hadoop.hive.ql.io.HiveInputFormat</value>
    </property>

    <!-- 动态分区 -->
    <property>
        <name>hive.exec.dynamic.partition.mode</name>
        <value>nonstrict</value>
    </property>

    <!-- hook 函数 -->
    <!--
         <property>
        <name>hive.exec.post.hooks</name>
        <value>org.apache.atlas.hive.hook.HiveHook</value>
    </property>
    -->
</configuration>
